{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import re\n",
    "\n",
    "def custom_tokenization(premise_hypothesis: Union[Tuple[str, str], List[str]], separator_marker: str=\"\", **tokenization_args) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Custom tokenization method that returns separate tokens for premise and hypothesis.\n",
    "\n",
    "    Args:\n",
    "        premise_hypothesis: Tuple or list containing (premise, hypothesis)\n",
    "        separator_marker: Special character(s) used by tokenizers when splitting words\n",
    "        tokenization_args: Additional tokenization arguments\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (premise_tokens, hypothesis_tokens)\n",
    "    \"\"\"\n",
    "    def _tokenize_text(text: str, lengths: List[int] = [1, 2, 3]) -> List[str]:\n",
    "        # Clean and split the text into words\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "\n",
    "        # Generate tokens for each length\n",
    "        for length in lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            # Loop through the words to create n-grams\n",
    "            for i in range(len(words) - length + 1):\n",
    "                token = ' '.join(words[i:i + length])\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Get lengths from tokenization_args if provided\n",
    "    lengths = tokenization_args.get('lengths', [1, 2, 3])\n",
    "\n",
    "    # Tokenize both premise and hypothesis\n",
    "    premise_tokens = _tokenize_text(premise_hypothesis[0], lengths)\n",
    "    hypothesis_tokens = _tokenize_text(premise_hypothesis[1], lengths)\n",
    "\n",
    "    return premise_tokens, hypothesis_tokens\n",
    "\n",
    "def custom_char_tokenization(premise_hypothesis: Union[Tuple[str, str], List[str]], separator_marker: str=\"\", **tokenization_args) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Custom character-level tokenization method.\n",
    "\n",
    "    Args:\n",
    "        premise_hypothesis: Tuple or list containing (premise, hypothesis)\n",
    "        separator_marker: Special character(s) used by tokenizers when splitting words\n",
    "        tokenization_args: Additional tokenization arguments\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (premise_char_tokens, hypothesis_char_tokens)\n",
    "    \"\"\"\n",
    "    def _tokenize_characters(text: str, lengths: List[int] = [2, 3]) -> List[str]:\n",
    "        text = text.replace(\" \", \"\")\n",
    "        tokens = []\n",
    "\n",
    "        for length in lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for i in range(len(text) - length + 1):\n",
    "                token = text[i:i + length]\n",
    "                if separator_marker and i > 0:\n",
    "                    token = f\"{separator_marker}{token}\"\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Get lengths from tokenization_args if provided\n",
    "    lengths = tokenization_args.get('lengths', [2, 3])\n",
    "\n",
    "    # Tokenize both premise and hypothesis\n",
    "    premise_tokens = _tokenize_characters(premise_hypothesis[0], lengths)\n",
    "    hypothesis_tokens = _tokenize_characters(premise_hypothesis[1], lengths)\n",
    "\n",
    "    return premise_tokens, hypothesis_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus:\n",
      "--------------------------------------------------\n",
      "\n",
      "Pair 1:\n",
      "Premise: The cat is sleeping on the mat.\n",
      "Hypothesis: There is a cat resting.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'cat', 'is', 'sleeping', 'on']\n",
      "Hypothesis: ['There', 'is', 'a', 'cat', 'resting.']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'ec', 'ca', 'at']\n",
      "Hypothesis: ['Th', 'he', 'er', 're', 'ei']\n",
      "\n",
      "Pair 2:\n",
      "Premise: Students attended the AI lecture.\n",
      "Hypothesis: People were learning about artificial intelligence.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['Students', 'attended', 'the', 'AI', 'lecture.']\n",
      "Hypothesis: ['People', 'were', 'learning', 'about', 'artificial']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['St', 'tu', 'ud', 'de', 'en']\n",
      "Hypothesis: ['Pe', 'eo', 'op', 'pl', 'le']\n",
      "\n",
      "Pair 3:\n",
      "Premise: The restaurant serves Italian food.\n",
      "Hypothesis: You can eat pasta at this place.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'restaurant', 'serves', 'Italian', 'food.']\n",
      "Hypothesis: ['You', 'can', 'eat', 'pasta', 'at']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'er', 're', 'es']\n",
      "Hypothesis: ['Yo', 'ou', 'uc', 'ca', 'an']\n",
      "\n",
      "Pair 4:\n",
      "Premise: It's raining heavily outside.\n",
      "Hypothesis: The weather is wet.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: [\"It's\", 'raining', 'heavily', 'outside.', \"It's raining\"]\n",
      "Hypothesis: ['The', 'weather', 'is', 'wet.', 'The weather']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['It', \"t'\", \"'s\", 'sr', 'ra']\n",
      "Hypothesis: ['Th', 'he', 'ew', 'we', 'ea']\n",
      "\n",
      "Pair 5:\n",
      "Premise: The computer is running slowly.\n",
      "Hypothesis: The system performance is poor.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'computer', 'is', 'running', 'slowly.']\n",
      "Hypothesis: ['The', 'system', 'performance', 'is', 'poor.']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'ec', 'co', 'om']\n",
      "Hypothesis: ['Th', 'he', 'es', 'sy', 'ys']\n"
     ]
    }
   ],
   "source": [
    "def create_test_corpus():\n",
    "    \"\"\"\n",
    "    Creates a small test corpus of premise-hypothesis pairs.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: List of (premise, hypothesis) pairs\n",
    "    \"\"\"\n",
    "    corpus = [\n",
    "        (\"The cat is sleeping on the mat.\", \"There is a cat resting.\"),\n",
    "        (\"Students attended the AI lecture.\", \"People were learning about artificial intelligence.\"),\n",
    "        (\"The restaurant serves Italian food.\", \"You can eat pasta at this place.\"),\n",
    "        (\"It's raining heavily outside.\", \"The weather is wet.\"),\n",
    "        (\"The computer is running slowly.\", \"The system performance is poor.\")\n",
    "    ]\n",
    "    return corpus\n",
    "\n",
    "# Test with the corpus\n",
    "corpus = create_test_corpus()\n",
    "\n",
    "print(\"\\nProcessing corpus:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (premise, hypothesis) in enumerate(corpus, 1):\n",
    "    print(f\"\\nPair {i}:\")\n",
    "    print(f\"Premise: {premise}\")\n",
    "    print(f\"Hypothesis: {hypothesis}\")\n",
    "\n",
    "    # Word-level tokenization\n",
    "    word_tokens_premise, word_tokens_hypothesis = custom_tokenization((premise, hypothesis))\n",
    "    print(f\"\\nWord-level tokens (first 5):\")\n",
    "    print(f\"Premise: {word_tokens_premise[:5]}\")\n",
    "    print(f\"Hypothesis: {word_tokens_hypothesis[:5]}\")\n",
    "\n",
    "    # Character-level tokenization\n",
    "    char_tokens_premise, char_tokens_hypothesis = custom_char_tokenization((premise, hypothesis))\n",
    "    print(f\"\\nCharacter-level tokens (first 5):\")\n",
    "    print(f\"Premise: {char_tokens_premise[:5]}\")\n",
    "    print(f\"Hypothesis: {char_tokens_hypothesis[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the tokenizer with the models Roberta / BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add these imports at the top\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the models and tokenizers\n",
    "roberta_model_name = \"roberta-base\"\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Initialize transformer tokenizers\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Initialize models\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_model_name).to(device)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "class CustomTokenizerWithTransformers(CustomTokenizer):\n",
    "    def __init__(self, lengths: List[int] = [1], transformer_tokenizer=None, transformer_model=None):\n",
    "        super().__init__(lengths)\n",
    "        self.transformer_tokenizer = transformer_tokenizer\n",
    "        self.transformer_model = transformer_model\n",
    "\n",
    "    def get_transformer_embeddings(self, text: str):\n",
    "        \"\"\"\n",
    "        Get embeddings from transformer model for the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Last hidden state embeddings\n",
    "        \"\"\"\n",
    "        if self.transformer_tokenizer is None or self.transformer_model is None:\n",
    "            raise ValueError(\"Transformer tokenizer and model must be set\")\n",
    "\n",
    "        # Tokenize text\n",
    "        inputs = self.transformer_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = self.transformer_model(**inputs)\n",
    "\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "# Example usage with both custom tokenization and transformer embeddings\n",
    "if __name__ == \"__main__\":\n",
    "    # Create tokenizers with transformers\n",
    "    roberta_custom_tokenizer = CustomTokenizerWithTransformers(\n",
    "        lengths=[1, 2, 3],\n",
    "        transformer_tokenizer=roberta_tokenizer,\n",
    "        transformer_model=roberta_model\n",
    "    )\n",
    "\n",
    "    bert_custom_tokenizer = CustomTokenizerWithTransformers(\n",
    "        lengths=[1, 2, 3],\n",
    "        transformer_tokenizer=bert_tokenizer,\n",
    "        transformer_model=bert_model\n",
    "    )\n",
    "\n",
    "    sample_text = \"Natural Language Inference is a task of NLI.\"\n",
    "\n",
    "    # Get custom tokenization\n",
    "    print(\"\\nCustom Word-Level Tokens:\", roberta_custom_tokenizer.tokenize(sample_text))\n",
    "    print(\"\\nCustom Character-Level Tokens:\", roberta_custom_tokenizer.tokenize_characters(sample_text))\n",
    "\n",
    "    # Get transformer embeddings\n",
    "    print(\"\\nGetting RoBERTa embeddings...\")\n",
    "    roberta_embeddings = roberta_custom_tokenizer.get_transformer_embeddings(sample_text)\n",
    "    print(f\"RoBERTa embedding shape: {roberta_embeddings.shape}\")\n",
    "\n",
    "    print(\"\\nGetting BERT embeddings...\")\n",
    "    bert_embeddings = bert_custom_tokenizer.get_transformer_embeddings(sample_text)\n",
    "    print(f\"BERT embedding shape: {bert_embeddings.shape}\")\n",
    "\n",
    "    # Process test corpus with both tokenization methods\n",
    "    corpus = create_test_corpus()\n",
    "    print(\"\\nProcessing corpus with both custom tokens and transformer embeddings:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, text in enumerate(corpus, 1):\n",
    "        print(f\"\\nText {i}: {text}\")\n",
    "\n",
    "        # Custom tokenization\n",
    "        word_tokens = roberta_custom_tokenizer.tokenize(text)\n",
    "        char_tokens = roberta_custom_tokenizer.tokenize_characters(text)\n",
    "\n",
    "        # Transformer embeddings\n",
    "        roberta_emb = roberta_custom_tokenizer.get_transformer_embeddings(text)\n",
    "        bert_emb = bert_custom_tokenizer.get_transformer_embeddings(text)\n",
    "\n",
    "        print(f\"Word tokens count: {len(word_tokens)}\")\n",
    "        print(f\"Char tokens count: {len(char_tokens)}\")\n",
    "        print(f\"RoBERTa embedding shape: {roberta_emb.shape}\")\n",
    "        print(f\"BERT embedding shape: {bert_emb.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
