{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Tokens: ['Natural', 'Language', 'Inference', 'is', 'a', 'task', 'of', 'NLI.', 'Natural Language', 'Language Inference', 'Inference is', 'is a', 'a task', 'task of', 'of NLI.', 'Natural Language Inference', 'Language Inference is', 'Inference is a', 'is a task', 'a task of', 'task of NLI.']\n",
      "Character-Level Tokens: ['Na', 'at', 'tu', 'ur', 'ra', 'al', 'lL', 'La', 'an', 'ng', 'gu', 'ua', 'ag', 'ge', 'eI', 'In', 'nf', 'fe', 'er', 're', 'en', 'nc', 'ce', 'ei', 'is', 'sa', 'at', 'ta', 'as', 'sk', 'ko', 'of', 'fN', 'NL', 'LI', 'I.', 'Nat', 'atu', 'tur', 'ura', 'ral', 'alL', 'lLa', 'Lan', 'ang', 'ngu', 'gua', 'uag', 'age', 'geI', 'eIn', 'Inf', 'nfe', 'fer', 'ere', 'ren', 'enc', 'nce', 'cei', 'eis', 'isa', 'sat', 'ata', 'tas', 'ask', 'sko', 'kof', 'ofN', 'fNL', 'NLI', 'LI.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, lengths: List[int] = [1]):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with desired token lengths.\n",
    "\n",
    "        Args:\n",
    "            lengths (List[int]): List of n-gram lengths to generate.\n",
    "                                 For example, [1, 2, 3] generates unigrams, bigrams, and trigrams.\n",
    "        \"\"\"\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into tokens of specified lengths.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of tokens of specified lengths.\n",
    "        \"\"\"\n",
    "        # Clean and split the text into words\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "\n",
    "        # Generate tokens for each length\n",
    "        for length in self.lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            # Loop through the words to create n-grams\n",
    "            for i in range(len(words) - length + 1):\n",
    "                token = ' '.join(words[i:i + length])\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_characters(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into character n-grams of specified lengths.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of character n-grams.\n",
    "        \"\"\"\n",
    "        text = text.replace(\" \", \"\")  # Remove spaces for character-level tokenization\n",
    "        tokens = []\n",
    "\n",
    "        # Generate character n-grams for each length\n",
    "        for length in self.lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for i in range(len(text) - length + 1):\n",
    "                token = text[i:i + length]\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize tokenizer for word-level tokens of length 1, 2, and 3\n",
    "    tokenizer = CustomTokenizer(lengths=[1, 2, 3])\n",
    "\n",
    "    sample_text = \"Natural Language Inference is a task of NLI.\"\n",
    "\n",
    "    print(\"Word-Level Tokens:\", tokenizer.tokenize(sample_text))\n",
    "\n",
    "    # Character-level n-grams of lengths 2 and 3\n",
    "    char_tokenizer = CustomTokenizer(lengths=[2, 3])\n",
    "    print(\"Character-Level Tokens:\", char_tokenizer.tokenize_characters(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text 1: Natural language processing is very cool.\n",
      "\n",
      "Word-level tokens (first 5): ['Natural', 'language', 'processing', 'is', 'very']\n",
      "Total word-level tokens: 15\n",
      "Character-level tokens (first 5): ['Na', 'at', 'tu', 'ur', 'ra']\n",
      "Total character-level tokens: 69\n",
      "\n",
      "Text 2: I do need to study a lot more.\n",
      "\n",
      "Word-level tokens (first 5): ['I', 'do', 'need', 'to', 'study']\n",
      "Total word-level tokens: 21\n",
      "Character-level tokens (first 5): ['Id', 'do', 'on', 'ne', 'ee']\n",
      "Total character-level tokens: 43\n",
      "\n",
      "Text 3: My neural network is not working.\n",
      "\n",
      "Word-level tokens (first 5): ['My', 'neural', 'network', 'is', 'not']\n",
      "Total word-level tokens: 15\n",
      "Character-level tokens (first 5): ['My', 'yn', 'ne', 'eu', 'ur']\n",
      "Total character-level tokens: 53\n",
      "\n",
      "Text 4: Garbage in, garbage out.\n",
      "\n",
      "Word-level tokens (first 5): ['Garbage', 'in,', 'garbage', 'out.', 'Garbage in,']\n",
      "Total word-level tokens: 9\n",
      "Character-level tokens (first 5): ['Ga', 'ar', 'rb', 'ba', 'ag']\n",
      "Total character-level tokens: 39\n",
      "\n",
      "Text 5: Please help me before the deadline.\n",
      "\n",
      "Word-level tokens (first 5): ['Please', 'help', 'me', 'before', 'the']\n",
      "Total word-level tokens: 15\n",
      "Character-level tokens (first 5): ['Pl', 'le', 'ea', 'as', 'se']\n",
      "Total character-level tokens: 57\n"
     ]
    }
   ],
   "source": [
    "# Add this code after the existing code\n",
    "\n",
    "def create_test_corpus():\n",
    "    \"\"\"\n",
    "    Creates a small test corpus of text data.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of text samples\n",
    "    \"\"\"\n",
    "    corpus = [\n",
    "        \"Natural language processing is very cool.\",\n",
    "        \"I do need to study a lot more.\",\n",
    "        \"My neural network is not working.\",\n",
    "        \"Garbage in, garbage out.\",\n",
    "        \"Please help me before the deadline.\"\n",
    "    ]\n",
    "    return corpus\n",
    "\n",
    "# Test with the corpus\n",
    "corpus = create_test_corpus()\n",
    "\n",
    "# Initialize tokenizers\n",
    "word_tokenizer = CustomTokenizer(lengths=[1, 2, 3])  # unigrams, bigrams, trigrams\n",
    "char_tokenizer = CustomTokenizer(lengths=[2, 3])     # character bigrams and trigrams\n",
    "\n",
    "# Process each text in the corpus\n",
    "print(\"\\nProcessing corpus:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, text in enumerate(corpus, 1):\n",
    "    print(f\"\\nText {i}: {text}\")\n",
    "\n",
    "    # Word-level tokenization\n",
    "    word_tokens = word_tokenizer.tokenize(text)\n",
    "    print(f\"\\nWord-level tokens (first 5):\", word_tokens[:5])\n",
    "    print(f\"Total word-level tokens:\", len(word_tokens))\n",
    "\n",
    "    # Character-level tokenization\n",
    "    char_tokens = char_tokenizer.tokenize_characters(text)\n",
    "    print(f\"Character-level tokens (first 5):\", char_tokens[:5])\n",
    "    print(f\"Total character-level tokens:\", len(char_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the tokenizer with the models Roberta / BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add these imports at the top\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the models and tokenizers\n",
    "roberta_model_name = \"roberta-base\"\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Initialize transformer tokenizers\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Initialize models\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_model_name).to(device)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "class CustomTokenizerWithTransformers(CustomTokenizer):\n",
    "    def __init__(self, lengths: List[int] = [1], transformer_tokenizer=None, transformer_model=None):\n",
    "        super().__init__(lengths)\n",
    "        self.transformer_tokenizer = transformer_tokenizer\n",
    "        self.transformer_model = transformer_model\n",
    "\n",
    "    def get_transformer_embeddings(self, text: str):\n",
    "        \"\"\"\n",
    "        Get embeddings from transformer model for the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Last hidden state embeddings\n",
    "        \"\"\"\n",
    "        if self.transformer_tokenizer is None or self.transformer_model is None:\n",
    "            raise ValueError(\"Transformer tokenizer and model must be set\")\n",
    "\n",
    "        # Tokenize text\n",
    "        inputs = self.transformer_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = self.transformer_model(**inputs)\n",
    "\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "# Example usage with both custom tokenization and transformer embeddings\n",
    "if __name__ == \"__main__\":\n",
    "    # Create tokenizers with transformers\n",
    "    roberta_custom_tokenizer = CustomTokenizerWithTransformers(\n",
    "        lengths=[1, 2, 3],\n",
    "        transformer_tokenizer=roberta_tokenizer,\n",
    "        transformer_model=roberta_model\n",
    "    )\n",
    "\n",
    "    bert_custom_tokenizer = CustomTokenizerWithTransformers(\n",
    "        lengths=[1, 2, 3],\n",
    "        transformer_tokenizer=bert_tokenizer,\n",
    "        transformer_model=bert_model\n",
    "    )\n",
    "\n",
    "    sample_text = \"Natural Language Inference is a task of NLI.\"\n",
    "\n",
    "    # Get custom tokenization\n",
    "    print(\"\\nCustom Word-Level Tokens:\", roberta_custom_tokenizer.tokenize(sample_text))\n",
    "    print(\"\\nCustom Character-Level Tokens:\", roberta_custom_tokenizer.tokenize_characters(sample_text))\n",
    "\n",
    "    # Get transformer embeddings\n",
    "    print(\"\\nGetting RoBERTa embeddings...\")\n",
    "    roberta_embeddings = roberta_custom_tokenizer.get_transformer_embeddings(sample_text)\n",
    "    print(f\"RoBERTa embedding shape: {roberta_embeddings.shape}\")\n",
    "\n",
    "    print(\"\\nGetting BERT embeddings...\")\n",
    "    bert_embeddings = bert_custom_tokenizer.get_transformer_embeddings(sample_text)\n",
    "    print(f\"BERT embedding shape: {bert_embeddings.shape}\")\n",
    "\n",
    "    # Process test corpus with both tokenization methods\n",
    "    corpus = create_test_corpus()\n",
    "    print(\"\\nProcessing corpus with both custom tokens and transformer embeddings:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, text in enumerate(corpus, 1):\n",
    "        print(f\"\\nText {i}: {text}\")\n",
    "\n",
    "        # Custom tokenization\n",
    "        word_tokens = roberta_custom_tokenizer.tokenize(text)\n",
    "        char_tokens = roberta_custom_tokenizer.tokenize_characters(text)\n",
    "\n",
    "        # Transformer embeddings\n",
    "        roberta_emb = roberta_custom_tokenizer.get_transformer_embeddings(text)\n",
    "        bert_emb = bert_custom_tokenizer.get_transformer_embeddings(text)\n",
    "\n",
    "        print(f\"Word tokens count: {len(word_tokens)}\")\n",
    "        print(f\"Char tokens count: {len(char_tokens)}\")\n",
    "        print(f\"RoBERTa embedding shape: {roberta_emb.shape}\")\n",
    "        print(f\"BERT embedding shape: {bert_emb.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
