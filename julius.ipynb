{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliusbijkerk/Documents/uulola/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model with Default Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model to load from Hugging Face\n",
    "model_name = \"bert-base-uncased\" \n",
    "\n",
    "# Load tokenizer: vocabulary and tokenization function\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Extract vocabulary from tokenizer\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n",
      "Sample vocabulary items:\n",
      "('soviets', 15269)\n",
      "('baltimore', 6222)\n",
      "('hysterical', 25614)\n",
      "('shrub', 15751)\n",
      "('##90', 21057)\n",
      "('tbs', 29584)\n",
      "('immensely', 24256)\n",
      "('greg', 6754)\n",
      "('claude', 8149)\n",
      "('1877', 7557)\n"
     ]
    }
   ],
   "source": [
    "# Expected size of vocabulary V = 30522\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Some examples of vocabulary items\n",
    "print(\"Sample vocabulary items:\")\n",
    "for item in list(vocab.items())[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Tokenization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Function: based on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenization_function(text):\n",
    "    \"\"\"\n",
    "    Tokenize the input text based on a custom tokenization function\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'premise': 'This church choir sings to the masses as they sing joyous songs from the book at a church.', 'hypothesis': 'The church has cracks in the ceiling.', 'label': 1}\n",
      "Premise tokens: ['This', 'church', 'choir', 'sings', 'to', 'the', 'masses', 'as', 'they', 'sing', 'joyous', 'songs', 'from', 'the', 'book', 'at', 'a', 'church.']\n",
      "Label: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "custom_tokenization_function() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Tokenize the first premise and hypothesis using the custom tokenizer\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m premise_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_tokenization_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_premises\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m hypothesis_tokens \u001b[38;5;241m=\u001b[39m custom_tokenization_function(test_hypotheses[\u001b[38;5;241m0\u001b[39m], vocab)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Display the tokenized premise and hypothesis\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: custom_tokenization_function() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "\n",
    "# Access train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Split test data into premises and hypotheses\n",
    "test_premises = test_data[\"premise\"]\n",
    "test_hypotheses = test_data[\"hypothesis\"]\n",
    "\n",
    "print(test_data)\n",
    "print(test_data[0])\n",
    "\n",
    "\n",
    "# Tokenize the first premise\n",
    "premise_tokens = custom_tokenization_function(test_premises[0])\n",
    "print(\"Premise tokens:\", premise_tokens)\n",
    "\n",
    "# Give the related label\n",
    "print(\"Label:\", test_data[\"label\"][0])\n",
    "\n",
    "\n",
    "# # Tokenize the first premise and hypothesis using the custom tokenizer\n",
    "# premise_tokens = custom_tokenization_function(test_premises[0], vocab)\n",
    "# hypothesis_tokens = custom_tokenization_function(test_hypotheses[0], vocab)\n",
    "\n",
    "# # Display the tokenized premise and hypothesis\n",
    "# print(\"Premise tokens:\", premise_tokens)\n",
    "# print(\"Hypothesis tokens:\", hypothesis_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show test_data information\n",
    "print(test_data)\n",
    "\n",
    "# Conversion table for labels\n",
    "conversion_table = {-1: '-1: no label', 0: '0: entailment', 1: '1: neutral', 2: '2: contradiction'}\n",
    "print(\"Label names:\")\n",
    "print(conversion_table)\n",
    "\n",
    "# Split test data into premises and hypotheses\n",
    "test_premises = test_data[\"premise\"]\n",
    "test_hypotheses = test_data[\"hypothesis\"]\n",
    "test_labels = test_data[\"label\"]\n",
    "\n",
    "# Display the first 5 examples \n",
    "zipped_data = list(zip(test_premises, test_hypotheses, test_labels))\n",
    "for example in zipped_data[:5]:\n",
    "    print(example)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(len(test_premises))\n",
    "print(len(test_hypotheses))\n",
    "print(len(test_labels))\n",
    "\n",
    "print(test_premises[0])\n",
    "print(test_hypotheses[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Lengths of train, validation, and test splits\n",
    "print(\"Train length:\", len(train_data))\n",
    "print(\"Validation length:\", len(validation_data))\n",
    "print(\"Test length:\", len(test_data))\n",
    "\n",
    "# Display the first example from the test set\n",
    "print(\"First 10 examples from the test set:\")\n",
    "print(test_data[:10])\n",
    "print(type(test_data[:10]))\n",
    "print(\"1st Example Premise:\", test_data[0]['premise'])\n",
    "print(\"1st Example Hypothesis:\", test_data[0]['hypothesis'])\n",
    "\n",
    "# Display the number of examples in the test set\n",
    "print(\"Number of examples in the test set:\", len(test_data))\n",
    "\n",
    "# Display the distribution of labels in the test set\n",
    "label_counts = test_data[\"label\"]\n",
    "\n",
    "# Count the frequency of each unique value\n",
    "counter = Counter(label_counts)\n",
    "\n",
    "# Extract the unique values and their counts\n",
    "labels, counts = zip(*counter.items())\n",
    "\n",
    "# Plot distribution of labels\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel('Unique Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Unique Values')\n",
    "labels_text = [conversion_table[label] for label in labels]\n",
    "plt.xticks(labels, labels_text)\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of sentence-length between premises and hypotheses\n",
    "premise_lengths = [len(premise.split()) for premise in test_data[\"premise\"]]\n",
    "hypothesis_lengths = [len(hypothesis.split()) for hypothesis in test_data[\"hypothesis\"]]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(premise_lengths, bins=50, alpha=0.6, label=\"Premises\")\n",
    "plt.hist(hypothesis_lengths, bins=50, alpha=0.6, label=\"Hypotheses\")\n",
    "plt.title(\"Length Distribution of Premises and Hypotheses in Test Set\")\n",
    "plt.xlabel(\"Length (number of words)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom tokenizer on snli testdata\n",
    "\n",
    "# tokens = custom_tokenizer(test_data, sample_vocab)\n",
    "# print(\"Tokens:\", tokens)\n",
    "# Apply custom tokenizer on all test data texts\n",
    "tokenized_test_data = [custom_tokenizer(text, sample_vocab) for text in test_data['premise']]\n",
    "print(\"Tokenized Test Data:\", tokenized_test_data[:5])  # Print first 5 tokenized examples for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Default Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the default tokenizer to all SNLI test data instances\n",
    "tokenized_test_data_default = [tokenizer.tokenize(text) for text in test_data['premise']]\n",
    "print(\"Tokenized Test Data (Default Tokenizer):\", tokenized_test_data_default[:5])  # Print first 5 tokenized examples for inspection\n",
    "\n",
    "print(test_data['premise'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Tokenizer Results: Custom vs. Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text, custom_vocab):\n",
    "    # Tokenize using the custom tokenizer\n",
    "    custom_tokens = custom_tokenizer(text, custom_vocab)\n",
    "    \n",
    "    # Tokenize using the pre-trained tokenizer\n",
    "    pretrained_tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Print both token lists for comparison\n",
    "    print(\"Custom Tokens:\", custom_tokens)\n",
    "    print(\"Pre-trained Tokens:\", pretrained_tokens)\n",
    "    \n",
    "    # Analyze the differences\n",
    "    if custom_tokens == pretrained_tokens:\n",
    "        print(\"Result: The tokenization is identical.\")\n",
    "    else:\n",
    "        print(\"Result: There are differences in tokenization.\")\n",
    "        print(\"Custom vs. Pre-trained:\")\n",
    "        for ct, pt in zip(custom_tokens, pretrained_tokens):\n",
    "            print(f\"{ct} -> {pt}\")\n",
    "\n",
    "    # Optionally, add more detailed analysis or statistics here\n",
    "    # e.g., token match rate, number of 'UNK' tokens, etc.\n",
    "\n",
    "# Example usage of the comparison function\n",
    "compare_tokenizers(\"hello world from Jupyter\", sample_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def token_accuracy(custom_tokens, pretrained_tokens):\n",
    "    correct = sum(ct == pt for ct, pt in zip(custom_tokens, pretrained_tokens))\n",
    "    total = len(pretrained_tokens)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def vocabulary_coverage(custom_tokens, pretrained_vocab):\n",
    "    covered = sum(token in pretrained_vocab for token in custom_tokens)\n",
    "    total = len(custom_tokens)\n",
    "    return covered / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def oov_rate(custom_tokens):\n",
    "    unk_tokens = custom_tokens.count('[UNK]')\n",
    "    total = len(custom_tokens)\n",
    "    return unk_tokens / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_f1_score(true_labels, predicted_labels):\n",
    "    return f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "\n",
    "def compare_tokenizers(text, custom_vocab, pretrained_vocab):\n",
    "    custom_tokens = custom_tokenizer(text, custom_vocab)\n",
    "    pretrained_tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    print(\"Custom Tokens:\", custom_tokens)\n",
    "    print(\"Pre-trained Tokens:\", pretrained_tokens)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = token_accuracy(custom_tokens, pretrained_tokens)\n",
    "    coverage = vocabulary_coverage(custom_tokens, pretrained_vocab)\n",
    "    oov = oov_rate(custom_tokens)\n",
    "    \n",
    "    print(f\"Token Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Vocabulary Coverage: {coverage:.2f}\")\n",
    "    print(f\"OOV Rate: {oov:.2f}\")\n",
    "\n",
    "    # More detailed comparison or additional metrics could be added here\n",
    "\n",
    "# Example usage\n",
    "pretrained_vocab = set(tokenizer.vocab.keys())  # Assuming Hugging Face Transformers\n",
    "compare_tokenizers(\"hello world from Jupyter\", sample_vocab, pretrained_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load the SNLI dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "\n",
    "# Load BERT's tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize data using BERT's tokenizer\n",
    "def bert_tokenize_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Tokenize the data with BERT's tokenizer\n",
    "encoded_dataset = dataset.map(bert_tokenize_function, batched=True)\n",
    "\n",
    "# Define a simple whitespace tokenizer function\n",
    "def whitespace_tokenize_function(examples):\n",
    "    # Use simple whitespace tokenization and manually map to BERT's vocabulary indices\n",
    "    premise_tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' '.join(word.split()))) for word in examples['premise']]\n",
    "    hypothesis_tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' '.join(word.split()))) for word in examples['hypothesis']]\n",
    "    return {'input_ids': premise_tokens, 'attention_mask': [[1] * len(tokens) for tokens in premise_tokens]}\n",
    "\n",
    "# Tokenize the data using the simple whitespace tokenizer\n",
    "encoded_dataset_whitespace = dataset.map(whitespace_tokenize_function, batched=True)\n",
    "\n",
    "# Initialize the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, preds), 'f1': np.mean(precision_recall_fscore_support(labels, preds, average='weighted'))}\n",
    "\n",
    "# Initialize the trainer for BERT tokenizer\n",
    "trainer_bert = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Initialize the trainer for whitespace tokenizer\n",
    "trainer_whitespace = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset_whitespace['train'],\n",
    "    eval_dataset=encoded_dataset_whitespace['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train and evaluate with BERT tokenizer\n",
    "print(\"Training with BERT tokenizer...\")\n",
    "trainer_bert.train()\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(\"Results with BERT tokenizer:\", results_bert)\n",
    "\n",
    "# Train and evaluate with whitespace tokenizer\n",
    "print(\"Training with whitespace tokenizer...\")\n",
    "trainer_whitespace.train()\n",
    "results_whitespace = trainer_whitespace.evaluate()\n",
    "print(\"Results with whitespace tokenizer:\", results_whitespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "snli_dataset = load_dataset('snli')\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = snli_dataset['train']\n",
    "validation_data = snli_dataset['validation']\n",
    "test_data = snli_dataset['test']\n",
    "\n",
    "# Example: Print the first example from the training set\n",
    "print(test_data[0])\n",
    "\n",
    "label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "# Example: Print the first example from the training set with label meaning\n",
    "example = train_data[0]\n",
    "example['label'] = label_mapping[example['label']]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\n",
    "if txt.strip():\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
