{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliusbijkerk/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/juliusbijkerk/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Comparison Model with its Default Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model that we want to load from Hugging Face\n",
    "model_name = \"bert-base-uncased\" \n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Extract the vocabulary from the tokenizer\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect the Initial Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n",
      "Sample vocabulary items:\n",
      "('realizes', 10919)\n",
      "('##rod', 14127)\n",
      "('triumph', 10911)\n",
      "('[unused578]', 583)\n",
      "('##tar', 7559)\n",
      "('storytelling', 20957)\n",
      "('##rcle', 21769)\n",
      "('knows', 4282)\n",
      "('recession', 19396)\n",
      "('weir', 16658)\n"
     ]
    }
   ],
   "source": [
    "# Display the vocabulary with expected size V of 30522\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample vocabulary items:\")\n",
    "for item in list(vocab.items())[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Tokenization Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text, vocab):\n",
    "    \"\"\"\n",
    "    Tokenize the input text based on a custom vocabulary.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize\n",
    "        vocab (set): A set of strings representing the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens\n",
    "    \"\"\"\n",
    "    # Example function that tokenizes based on whitespace\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "dataset = load_dataset('snli')\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Split test data into premises and hypotheses\n",
    "test_premises = test_data['premise']\n",
    "test_hypotheses = test_data['hypothesis']\n",
    "\n",
    "# Tokenize the first premise and hypothesis\n",
    "premise_tokens = custom_tokenizer(test_premises[0], vocab)\n",
    "hypothesis_tokens = custom_tokenizer(test_hypotheses[0], vocab)\n",
    "\n",
    "# Display the tokenized premise and hypothesis\n",
    "print(\"Premise tokens:\", premise_tokens)\n",
    "print(\"Hypothesis tokens:\", hypothesis_tokens)\n",
    "\n",
    "# Tokenize the first premise and hypothesis using the custom tokenizer\n",
    "premise_tokens = custom_tokenizer(test_premises[0], vocab)\n",
    "hypothesis_tokens = custom_tokenizer(test_hypotheses[0], vocab)\n",
    "\n",
    "# Display the tokenized premise and hypothesis\n",
    "print(\"Premise tokens:\", premise_tokens)\n",
    "print(\"Hypothesis tokens:\", hypothesis_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 examples from the test set:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the first example from the test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 10 examples from the test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_data\u001b[49m[:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(test_data[:\u001b[38;5;241m10\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st Example Premise:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Display the first example from the test set\n",
    "print(\"First 10 examples from the test set:\")\n",
    "print(test_data[:10])\n",
    "print(type(test_data[:10]))\n",
    "print(\"1st Example Premise:\", test_data[0]['premise'])\n",
    "print(\"1st Example Hypothesis:\", test_data[0]['hypothesis'])\n",
    "\n",
    "# Display the number of examples in the test set\n",
    "print(\"Number of examples in the test set:\", len(test_data))\n",
    "\n",
    "# Display conversion table for labels\n",
    "print(\"Conversion table for labels:\")\n",
    "print(dataset['train'].features['label'].str2int)\n",
    "\n",
    "# Conversion table for labels\n",
    "label_names = {1: 'entailment', 0: 'neutral', 2: 'contradiction', -1: 'unknown'}\n",
    "print(\"Label names:\")\n",
    "print(label_names)\n",
    "\n",
    "# Display the distribution of labels in the test set\n",
    "label_counts = test_data[\"label\"]\n",
    "\n",
    "# Count the frequency of each unique value\n",
    "counter = Counter(label_counts)\n",
    "\n",
    "# Extract the unique values and their counts\n",
    "labels, counts = zip(*counter.items())\n",
    "\n",
    "# Plot the distribution\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Unique Values')\n",
    "plt.xticks(labels)  # Ensure x-axis has discrete integer values\n",
    "plt.show()\n",
    "\n",
    "# # Display the length distribution of premises and hypotheses\n",
    "# premise_lengths = [len(premise.split()) for premise in test_data[\"premise\"]]\n",
    "# hypothesis_lengths = [len(hypothesis.split()) for hypothesis in test_data[\"hypothesis\"]]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.hist(premise_lengths, bins=50, alpha=0.6, label=\"Premises\")\n",
    "# plt.hist(hypothesis_lengths, bins=50, alpha=0.6, label=\"Hypotheses\")\n",
    "# plt.title(\"Length Distribution of Premises and Hypotheses in Test Set\")\n",
    "# plt.xlabel(\"Length (number of words)\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Test Data: [['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'], ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'], ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'], ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'], ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']]\n"
     ]
    }
   ],
   "source": [
    "# Apply custom tokenizer on snli testdata\n",
    "\n",
    "# tokens = custom_tokenizer(test_data, sample_vocab)\n",
    "# print(\"Tokens:\", tokens)\n",
    "# Apply custom tokenizer on all test data texts\n",
    "tokenized_test_data = [custom_tokenizer(text, sample_vocab) for text in test_data['premise']]\n",
    "print(\"Tokenized Test Data:\", tokenized_test_data[:5])  # Print first 5 tokenized examples for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Default Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Test Data (Default Tokenizer): [['this', 'church', 'choir', 'sings', 'to', 'the', 'masses', 'as', 'they', 'sing', 'joy', '##ous', 'songs', 'from', 'the', 'book', 'at', 'a', 'church', '.'], ['this', 'church', 'choir', 'sings', 'to', 'the', 'masses', 'as', 'they', 'sing', 'joy', '##ous', 'songs', 'from', 'the', 'book', 'at', 'a', 'church', '.'], ['this', 'church', 'choir', 'sings', 'to', 'the', 'masses', 'as', 'they', 'sing', 'joy', '##ous', 'songs', 'from', 'the', 'book', 'at', 'a', 'church', '.'], ['a', 'woman', 'with', 'a', 'green', 'heads', '##car', '##f', ',', 'blue', 'shirt', 'and', 'a', 'very', 'big', 'grin', '.'], ['a', 'woman', 'with', 'a', 'green', 'heads', '##car', '##f', ',', 'blue', 'shirt', 'and', 'a', 'very', 'big', 'grin', '.']]\n",
      "['This church choir sings to the masses as they sing joyous songs from the book at a church.', 'This church choir sings to the masses as they sing joyous songs from the book at a church.', 'This church choir sings to the masses as they sing joyous songs from the book at a church.', 'A woman with a green headscarf, blue shirt and a very big grin.', 'A woman with a green headscarf, blue shirt and a very big grin.']\n"
     ]
    }
   ],
   "source": [
    "# Apply the default tokenizer to all SNLI test data instances\n",
    "tokenized_test_data_default = [tokenizer.tokenize(text) for text in test_data['premise']]\n",
    "print(\"Tokenized Test Data (Default Tokenizer):\", tokenized_test_data_default[:5])  # Print first 5 tokenized examples for inspection\n",
    "\n",
    "print(test_data['premise'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Tokenizer Results: Custom vs. Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text, custom_vocab):\n",
    "    # Tokenize using the custom tokenizer\n",
    "    custom_tokens = custom_tokenizer(text, custom_vocab)\n",
    "    \n",
    "    # Tokenize using the pre-trained tokenizer\n",
    "    pretrained_tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Print both token lists for comparison\n",
    "    print(\"Custom Tokens:\", custom_tokens)\n",
    "    print(\"Pre-trained Tokens:\", pretrained_tokens)\n",
    "    \n",
    "    # Analyze the differences\n",
    "    if custom_tokens == pretrained_tokens:\n",
    "        print(\"Result: The tokenization is identical.\")\n",
    "    else:\n",
    "        print(\"Result: There are differences in tokenization.\")\n",
    "        print(\"Custom vs. Pre-trained:\")\n",
    "        for ct, pt in zip(custom_tokens, pretrained_tokens):\n",
    "            print(f\"{ct} -> {pt}\")\n",
    "\n",
    "    # Optionally, add more detailed analysis or statistics here\n",
    "    # e.g., token match rate, number of 'UNK' tokens, etc.\n",
    "\n",
    "# Example usage of the comparison function\n",
    "compare_tokenizers(\"hello world from Jupyter\", sample_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def token_accuracy(custom_tokens, pretrained_tokens):\n",
    "    correct = sum(ct == pt for ct, pt in zip(custom_tokens, pretrained_tokens))\n",
    "    total = len(pretrained_tokens)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def vocabulary_coverage(custom_tokens, pretrained_vocab):\n",
    "    covered = sum(token in pretrained_vocab for token in custom_tokens)\n",
    "    total = len(custom_tokens)\n",
    "    return covered / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def oov_rate(custom_tokens):\n",
    "    unk_tokens = custom_tokens.count('[UNK]')\n",
    "    total = len(custom_tokens)\n",
    "    return unk_tokens / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_f1_score(true_labels, predicted_labels):\n",
    "    return f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "\n",
    "def compare_tokenizers(text, custom_vocab, pretrained_vocab):\n",
    "    custom_tokens = custom_tokenizer(text, custom_vocab)\n",
    "    pretrained_tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    print(\"Custom Tokens:\", custom_tokens)\n",
    "    print(\"Pre-trained Tokens:\", pretrained_tokens)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = token_accuracy(custom_tokens, pretrained_tokens)\n",
    "    coverage = vocabulary_coverage(custom_tokens, pretrained_vocab)\n",
    "    oov = oov_rate(custom_tokens)\n",
    "    \n",
    "    print(f\"Token Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Vocabulary Coverage: {coverage:.2f}\")\n",
    "    print(f\"OOV Rate: {oov:.2f}\")\n",
    "\n",
    "    # More detailed comparison or additional metrics could be added here\n",
    "\n",
    "# Example usage\n",
    "pretrained_vocab = set(tokenizer.vocab.keys())  # Assuming Hugging Face Transformers\n",
    "compare_tokenizers(\"hello world from Jupyter\", sample_vocab, pretrained_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load the SNLI dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "\n",
    "# Load BERT's tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize data using BERT's tokenizer\n",
    "def bert_tokenize_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Tokenize the data with BERT's tokenizer\n",
    "encoded_dataset = dataset.map(bert_tokenize_function, batched=True)\n",
    "\n",
    "# Define a simple whitespace tokenizer function\n",
    "def whitespace_tokenize_function(examples):\n",
    "    # Use simple whitespace tokenization and manually map to BERT's vocabulary indices\n",
    "    premise_tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' '.join(word.split()))) for word in examples['premise']]\n",
    "    hypothesis_tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' '.join(word.split()))) for word in examples['hypothesis']]\n",
    "    return {'input_ids': premise_tokens, 'attention_mask': [[1] * len(tokens) for tokens in premise_tokens]}\n",
    "\n",
    "# Tokenize the data using the simple whitespace tokenizer\n",
    "encoded_dataset_whitespace = dataset.map(whitespace_tokenize_function, batched=True)\n",
    "\n",
    "# Initialize the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, preds), 'f1': np.mean(precision_recall_fscore_support(labels, preds, average='weighted'))}\n",
    "\n",
    "# Initialize the trainer for BERT tokenizer\n",
    "trainer_bert = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Initialize the trainer for whitespace tokenizer\n",
    "trainer_whitespace = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset_whitespace['train'],\n",
    "    eval_dataset=encoded_dataset_whitespace['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train and evaluate with BERT tokenizer\n",
    "print(\"Training with BERT tokenizer...\")\n",
    "trainer_bert.train()\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(\"Results with BERT tokenizer:\", results_bert)\n",
    "\n",
    "# Train and evaluate with whitespace tokenizer\n",
    "print(\"Training with whitespace tokenizer...\")\n",
    "trainer_whitespace.train()\n",
    "results_whitespace = trainer_whitespace.evaluate()\n",
    "print(\"Results with whitespace tokenizer:\", results_whitespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "snli_dataset = load_dataset('snli')\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = snli_dataset['train']\n",
    "validation_data = snli_dataset['validation']\n",
    "test_data = snli_dataset['test']\n",
    "\n",
    "# Example: Print the first example from the training set\n",
    "print(test_data[0])\n",
    "\n",
    "label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "# Example: Print the first example from the training set with label meaning\n",
    "example = train_data[0]\n",
    "example['label'] = label_mapping[example['label']]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\n",
    "if txt.strip():\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
