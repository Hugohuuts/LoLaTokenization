{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to check that all tokenization methods don't exceed the maximum context window of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from method_mapping import *\n",
    "from custom_tokenizer_abstract import *\n",
    "from custom_models import load_custom_class\n",
    "\n",
    "data_paths = [\n",
    "    \"data/multinli_1.0/multinli_1.0_dev_matched.jsonl\",\n",
    "    \"data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\",\n",
    "    \"data/snli_1.0/snli_1.0_test.jsonl\",\n",
    "]\n",
    "data = {\n",
    "        \"label\": [],\n",
    "        \"sent1\": [],\n",
    "        \"sent2\": []\n",
    "    }\n",
    "for data_path in data_paths:\n",
    "    with open(data_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            aux_dict = json.loads(line)\n",
    "            data[\"label\"] += [aux_dict[\"gold_label\"]]\n",
    "            data[\"sent1\"] += [aux_dict[\"sentence1\"]] # premise\n",
    "            data[\"sent2\"] += [aux_dict[\"sentence2\"]] # hypothesis\n",
    "\n",
    "    for tok_method in TOK_METHOD_MAP.values():\n",
    "        for model_name, model_parameters in MODEL_MAP.items():\n",
    "            tokenizer_nli = load_custom_class(model_parameters[\"model_link\"], load_model=False)\n",
    "            custom_tokenizer = CustomTokenizerGeneral(tokenizer_nli, tok_method, separator=model_parameters[\"separator_marker\"], special_space_token=model_parameters[\"special_space_token\"], max_length=model_parameters[\"max_length\"])\n",
    "            \n",
    "            total = len(data[\"sent1\"])\n",
    "            for premise, hypothesis in tqdm(zip(data[\"sent1\"], data[\"sent2\"]), total=total, desc=f\"{model_name} --- {tok_method.__name__} -- {data_path.split('/')[-1]}\"):\n",
    "                tok_ids = custom_tokenizer((premise, hypothesis))[\"input_ids\"]\n",
    "                assert len(tok_ids) <= model_parameters[\"max_length\"], f\"More than {model_parameters['max_length']} tokens; observed {len(tok_ids)} tokens.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "* DistilRoBERTa, BART, and MiniLM2, while displaying tokens with a blank space in front, actually use \"Ä \" in the vocabulary instead of \" \" - which we will call <tt>special_space_token</tt>.\n",
    "* These same models also have separation indicators (which we term as <tt>separation_marker</tt>), indicating which subwords were part of an original word (e.g., \"dog\" -> \"d ##og\"); though, the mentioned models do not indicate this separation, which we represent thus as \"\" and not modify the strings further.\n",
    "* DistilRoBERTa and MiniLM2 have the same type of tokenizer processor (based on RoBERTa), while BART has its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'This', ' is', ' a', ' test', ' sentence', ' to', ' showcase', '</s>', '</s>', 'How', ' the', ' token', 'izers', ' running', ' native', 'ly', ' separate', ' tokens', ' stylish', 'ly', '.', '</s>']\n",
      "['<s>', 'This', ' is', ' a', ' test', ' sentence', ' to', ' showcase', '</s>', '</s>', 'How', ' the', ' token', 'izers', ' running', ' native', 'ly', ' separate', ' tokens', ' stylish', 'ly', '.', '</s>']\n",
      "['<s>', 'This', ' is', ' a', ' test', ' sentence', ' to', ' showcase', '</s>', '</s>', 'How', ' the', ' token', 'izers', ' running', ' native', 'ly', ' separate', ' tokens', ' stylish', 'ly', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = (\"This is a test sentence to showcase\", \"How the tokenizers running natively separate tokens stylishly.\")\n",
    "for model_name, model_parameters in MODEL_MAP.items():\n",
    "    tokenizer_nli: AutoTokenizer = load_custom_class(model_parameters[\"model_link\"], load_model=False)\n",
    "    print([tokenizer_nli.decode(token) for token in tokenizer_nli.encode(*test_sentence)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
