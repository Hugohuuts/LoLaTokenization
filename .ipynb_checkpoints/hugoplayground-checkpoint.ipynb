{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Union\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_tokenization\u001b[39m(premise_hypothesis: Union[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m]], separator_marker: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenization_args) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m]]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n",
    "import torch\n",
    "\n",
    "def custom_tokenization(premise_hypothesis: Union[Tuple[str, str], List[str]], separator_marker: str=\"\", **tokenization_args) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Custom tokenization method that returns separate tokens for premise and hypothesis.\n",
    "\n",
    "    Args:\n",
    "        premise_hypothesis: Tuple or list containing (premise, hypothesis)\n",
    "        separator_marker: Special character(s) used by tokenizers when splitting words\n",
    "        tokenization_args: Additional tokenization arguments\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (premise_tokens, hypothesis_tokens)\n",
    "    \"\"\"\n",
    "    def _tokenize_text(text: str, lengths: List[int] = [1, 2, 3]) -> List[str]:\n",
    "        # Clean and split the text into words\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "\n",
    "        # Generate tokens for each length\n",
    "        for length in lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            # Loop through the words to create n-grams\n",
    "            for i in range(len(words) - length + 1):\n",
    "                token = ' '.join(words[i:i + length])\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Get lengths from tokenization_args if provided\n",
    "    lengths = tokenization_args.get('lengths', [1, 2, 3])\n",
    "\n",
    "    # Tokenize both premise and hypothesis\n",
    "    premise_tokens = _tokenize_text(premise_hypothesis[0], lengths)\n",
    "    hypothesis_tokens = _tokenize_text(premise_hypothesis[1], lengths)\n",
    "\n",
    "    return premise_tokens, hypothesis_tokens\n",
    "\n",
    "def custom_char_tokenization(premise_hypothesis: Union[Tuple[str, str], List[str]], separator_marker: str=\"\", **tokenization_args) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Custom character-level tokenization method.\n",
    "\n",
    "    Args:\n",
    "        premise_hypothesis: Tuple or list containing (premise, hypothesis)\n",
    "        separator_marker: Special character(s) used by tokenizers when splitting words\n",
    "        tokenization_args: Additional tokenization arguments\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (premise_char_tokens, hypothesis_char_tokens)\n",
    "    \"\"\"\n",
    "    def _tokenize_characters(text: str, lengths: List[int] = [2, 3]) -> List[str]:\n",
    "        text = text.replace(\" \", \"\")\n",
    "        tokens = []\n",
    "\n",
    "        for length in lengths:\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for i in range(len(text) - length + 1):\n",
    "                token = text[i:i + length]\n",
    "                if separator_marker and i > 0:\n",
    "                    token = f\"{separator_marker}{token}\"\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Get lengths from tokenization_args if provided\n",
    "    lengths = tokenization_args.get('lengths', [2, 3])\n",
    "\n",
    "    # Tokenize both premise and hypothesis\n",
    "    premise_tokens = _tokenize_characters(premise_hypothesis[0], lengths)\n",
    "    hypothesis_tokens = _tokenize_characters(premise_hypothesis[1], lengths)\n",
    "\n",
    "    return premise_tokens, hypothesis_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair 1:\n",
      "Premise: The cat is sleeping on the mat.\n",
      "Hypothesis: There is a cat resting.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'cat', 'is', 'sleeping', 'on']\n",
      "Hypothesis: ['There', 'is', 'a', 'cat', 'resting.']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'ec', 'ca', 'at']\n",
      "Hypothesis: ['Th', 'he', 'er', 're', 'ei']\n",
      "\n",
      "Golden-chunk tokens (first 5):\n",
      "Premise: ['Th', 'eca', 'tiss', 'leepin', 'go']\n",
      "Hypothesis: ['Th', 'ere', 'isac', 'atrest', 'in']\n",
      "\n",
      "Pair 2:\n",
      "Premise: Students attended the AI lecture.\n",
      "Hypothesis: People were learning about artificial intelligence.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['Students', 'attended', 'the', 'AI', 'lecture.']\n",
      "Hypothesis: ['People', 'were', 'learning', 'about', 'artificial']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['St', 'tu', 'ud', 'de', 'en']\n",
      "Hypothesis: ['Pe', 'eo', 'op', 'pl', 'le']\n",
      "\n",
      "Golden-chunk tokens (first 5):\n",
      "Premise: ['St', 'ude', 'ntsa', 'ttende', 'dt']\n",
      "Hypothesis: ['Pe', 'opl', 'ewer', 'elearn', 'in']\n",
      "\n",
      "Pair 3:\n",
      "Premise: The restaurant serves Italian food.\n",
      "Hypothesis: You can eat pasta at this place.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'restaurant', 'serves', 'Italian', 'food.']\n",
      "Hypothesis: ['You', 'can', 'eat', 'pasta', 'at']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'er', 're', 'es']\n",
      "Hypothesis: ['Yo', 'ou', 'uc', 'ca', 'an']\n",
      "\n",
      "Golden-chunk tokens (first 5):\n",
      "Premise: ['Th', 'ere', 'stau', 'rantse', 'rv']\n",
      "Hypothesis: ['Yo', 'uca', 'neat', 'pastaa', 'tt']\n",
      "\n",
      "Pair 4:\n",
      "Premise: It's raining heavily outside.\n",
      "Hypothesis: The weather is wet.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: [\"It's\", 'raining', 'heavily', 'outside.', \"It's raining\"]\n",
      "Hypothesis: ['The', 'weather', 'is', 'wet.', 'The weather']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['It', \"t'\", \"'s\", 'sr', 'ra']\n",
      "Hypothesis: ['Th', 'he', 'ew', 'we', 'ea']\n",
      "\n",
      "Golden-chunk tokens (first 5):\n",
      "Premise: ['It', \"'sr\", 'aini', 'ngheav', 'il']\n",
      "Hypothesis: ['Th', 'ewe', 'athe', 'riswet', '.']\n",
      "\n",
      "Pair 5:\n",
      "Premise: The computer is running slowly.\n",
      "Hypothesis: The system performance is poor.\n",
      "\n",
      "Word-level tokens (first 5):\n",
      "Premise: ['The', 'computer', 'is', 'running', 'slowly.']\n",
      "Hypothesis: ['The', 'system', 'performance', 'is', 'poor.']\n",
      "\n",
      "Character-level tokens (first 5):\n",
      "Premise: ['Th', 'he', 'ec', 'co', 'om']\n",
      "Hypothesis: ['Th', 'he', 'es', 'sy', 'ys']\n",
      "\n",
      "Golden-chunk tokens (first 5):\n",
      "Premise: ['Th', 'eco', 'mput', 'erisru', 'nn']\n",
      "Hypothesis: ['Th', 'esy', 'stem', 'perfor', 'ma']\n"
     ]
    }
   ],
   "source": [
    "def create_test_corpus():\n",
    "    \"\"\"\n",
    "    Creates a small test corpus of premise-hypothesis pairs.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: List of (premise, hypothesis) pairs\n",
    "    \"\"\"\n",
    "    corpus = [\n",
    "        (\"The cat is sleeping on the mat.\", \"There is a cat resting.\"),\n",
    "        (\"Students attended the AI lecture.\", \"People were learning about artificial intelligence.\"),\n",
    "        (\"The restaurant serves Italian food.\", \"You can eat pasta at this place.\"),\n",
    "        (\"It's raining heavily outside.\", \"The weather is wet.\"),\n",
    "        (\"The computer is running slowly.\", \"The system performance is poor.\")\n",
    "    ]\n",
    "    return corpus\n",
    "\n",
    "# Test with the corpus\n",
    "corpus = create_test_corpus()\n",
    "\n",
    "\n",
    "\n",
    "for i, (premise, hypothesis) in enumerate(corpus, 1):\n",
    "    print(f\"\\nPair {i}:\")\n",
    "    print(f\"Premise: {premise}\")\n",
    "    print(f\"Hypothesis: {hypothesis}\")\n",
    "\n",
    "    # Word-level tokenization\n",
    "    word_tokens_premise, word_tokens_hypothesis = custom_tokenization((premise, hypothesis))\n",
    "    print(f\"\\nWord-level tokens (first 5):\")\n",
    "    print(f\"Premise: {word_tokens_premise[:5]}\")\n",
    "    print(f\"Hypothesis: {word_tokens_hypothesis[:5]}\")\n",
    "\n",
    "    # Character-level tokenization\n",
    "    char_tokens_premise, char_tokens_hypothesis = custom_char_tokenization((premise, hypothesis))\n",
    "    print(f\"\\nCharacter-level tokens (first 5):\")\n",
    "    print(f\"Premise: {char_tokens_premise[:5]}\")\n",
    "    print(f\"Hypothesis: {char_tokens_hypothesis[:5]}\")\n",
    "\n",
    "    # Golden-chunk tokenization\n",
    "    golden_tokens_premise, golden_tokens_hypothesis = golden_chunk_tokenization(\n",
    "        (premise, hypothesis),\n",
    "        initial_length=2,\n",
    "        ratio=1.618,\n",
    "        max_chunk_length=8,\n",
    "        rounding_mode='floor',\n",
    "        reset_at_max=True\n",
    "    )\n",
    "    print(f\"\\nGolden-chunk tokens (first 5):\")\n",
    "    print(f\"Premise: {golden_tokens_premise[:5]}\")\n",
    "    print(f\"Hypothesis: {golden_tokens_hypothesis[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the tokenizer with the models Roberta / BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_chunk_tokenization(premise_hypothesis: Union[Tuple[str, str], List[str]], **tokenization_args) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Golden-Chunk tokenization method that uses increasing chunk sizes based on the golden ratio.\n",
    "\n",
    "    Args:\n",
    "        premise_hypothesis: Tuple or list containing (premise, hypothesis)\n",
    "        tokenization_args: Additional arguments including:\n",
    "            - initial_length: Starting chunk size (default: 2)\n",
    "            - ratio: Growth ratio (default: 1.618)\n",
    "            - max_chunk_length: Maximum chunk size (default: 8)\n",
    "            - rounding_mode: How to round chunk sizes ('floor', 'ceil', 'round') (default: 'floor')\n",
    "            - reset_at_max: Whether to reset chunk size after hitting max (default: True)\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (premise_tokens, hypothesis_tokens)\n",
    "    \"\"\"\n",
    "    def _golden_chunk_text(text: str, **args) -> List[str]:\n",
    "        # Initialize parameters\n",
    "        initial_length = args.get('initial_length', 2)\n",
    "        ratio = args.get('ratio', 1.618)\n",
    "        max_chunk_length = args.get('max_chunk_length', 8)\n",
    "        rounding_mode = args.get('rounding_mode', 'floor')\n",
    "        reset_at_max = args.get('reset_at_max', True)\n",
    "\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        current_length = initial_length\n",
    "        text = text.replace(\" \", \"\")  # Remove spaces\n",
    "\n",
    "        while i < len(text):\n",
    "            # Extract chunk\n",
    "            chunk = text[i:i + current_length]\n",
    "            if chunk:  # Only add non-empty chunks\n",
    "                tokens.append(chunk)\n",
    "\n",
    "            # Move pointer\n",
    "            i += current_length\n",
    "\n",
    "            # Calculate next chunk length\n",
    "            next_length = current_length * ratio\n",
    "            if rounding_mode == 'floor':\n",
    "                next_length = int(next_length)\n",
    "            elif rounding_mode == 'ceil':\n",
    "                next_length = math.ceil(next_length)\n",
    "            else:  # 'round'\n",
    "                next_length = round(next_length)\n",
    "\n",
    "            # Apply maximum length constraint\n",
    "            if next_length > max_chunk_length:\n",
    "                current_length = initial_length if reset_at_max else max_chunk_length\n",
    "            else:\n",
    "                current_length = max(1, next_length)  # Ensure at least length 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Apply tokenization to both premise and hypothesis\n",
    "    premise_tokens = _golden_chunk_text(premise_hypothesis[0], **tokenization_args)\n",
    "    hypothesis_tokens = _golden_chunk_text(premise_hypothesis[1], **tokenization_args)\n",
    "\n",
    "    return premise_tokens, hypothesis_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_transformers\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, RobertaModel, BertModel\n",
    "import torch\n",
    "\n",
    "def initialize_transformers():\n",
    "    \"\"\"\n",
    "    Initialize transformer models and tokenizers.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (roberta_tokenizer, bert_tokenizer, roberta_model, bert_model, device)\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize the models and tokenizers\n",
    "    roberta_model_name = \"roberta-base\"\n",
    "    bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "    print(\"\\nLoading tokenizers...\")\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "    print(\"Loading models...\")\n",
    "    roberta_model = RobertaModel.from_pretrained(roberta_model_name).to(device)\n",
    "    bert_model = BertModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "    # Set models to evaluation mode\n",
    "    roberta_model.eval()\n",
    "    bert_model.eval()\n",
    "\n",
    "    return roberta_tokenizer, bert_tokenizer, roberta_model, bert_model, device\n",
    "\n",
    "def process_text_with_transformers(text: str, tokenizers, models, device):\n",
    "    \"\"\"\n",
    "    Process text with different tokenization methods and transformer models.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        tokenizers (tuple): (roberta_tokenizer, bert_tokenizer)\n",
    "        models (tuple): (roberta_model, bert_model)\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        dict: Results including tokens and embeddings\n",
    "    \"\"\"\n",
    "    roberta_tokenizer, bert_tokenizer = tokenizers\n",
    "    roberta_model, bert_model = models\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Word-level tokenization\n",
    "    word_tokens, _ = custom_tokenization((text, \"\"))\n",
    "    results['word_tokens'] = word_tokens[:5]  # First 5 tokens\n",
    "\n",
    "    # Character-level tokenization\n",
    "    char_tokens, _ = custom_char_tokenization((text, \"\"))\n",
    "    results['char_tokens'] = char_tokens[:5]  # First 5 tokens\n",
    "\n",
    "    # Golden-chunk tokenization\n",
    "    golden_tokens, _ = golden_chunk_tokenization(\n",
    "        (text, \"\"),\n",
    "        initial_length=2,\n",
    "        ratio=1.618,\n",
    "        max_chunk_length=8,\n",
    "        rounding_mode='floor',\n",
    "        reset_at_max=True\n",
    "    )\n",
    "    results['golden_tokens'] = golden_tokens[:5]  # First 5 tokens\n",
    "\n",
    "    # Get transformer embeddings\n",
    "    with torch.no_grad():\n",
    "        # RoBERTa\n",
    "        roberta_inputs = roberta_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        roberta_outputs = roberta_model(**roberta_inputs)\n",
    "        results['roberta_embeddings'] = roberta_outputs.last_hidden_state\n",
    "\n",
    "        # BERT\n",
    "        bert_inputs = bert_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        bert_outputs = bert_model(**bert_inputs)\n",
    "        results['bert_embeddings'] = bert_outputs.last_hidden_state\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize transformers\n",
    "    print(\"Initializing transformer models...\")\n",
    "    roberta_tokenizer, bert_tokenizer, roberta_model, bert_model, device = initialize_transformers()\n",
    "\n",
    "    # Create test corpus\n",
    "    corpus = create_test_corpus()\n",
    "\n",
    "    print(\"\\nProcessing corpus with all tokenization methods and transformers:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, (premise, hypothesis) in enumerate(corpus, 1):\n",
    "        print(f\"\\nPair {i}:\")\n",
    "        print(f\"Premise: {premise}\")\n",
    "        print(f\"Hypothesis: {hypothesis}\")\n",
    "\n",
    "        # Process premise\n",
    "        premise_results = process_text_with_transformers(\n",
    "            premise,\n",
    "            (roberta_tokenizer, bert_tokenizer),\n",
    "            (roberta_model, bert_model),\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Process hypothesis\n",
    "        hypothesis_results = process_text_with_transformers(\n",
    "            hypothesis,\n",
    "            (roberta_tokenizer, bert_tokenizer),\n",
    "            (roberta_model, bert_model),\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nPremise processing:\")\n",
    "        print(f\"Word tokens: {premise_results['word_tokens']}\")\n",
    "        print(f\"Char tokens: {premise_results['char_tokens']}\")\n",
    "        print(f\"Golden tokens: {premise_results['golden_tokens']}\")\n",
    "        print(f\"RoBERTa embedding shape: {premise_results['roberta_embeddings'].shape}\")\n",
    "        print(f\"BERT embedding shape: {premise_results['bert_embeddings'].shape}\")\n",
    "\n",
    "        print(\"\\nHypothesis processing:\")\n",
    "        print(f\"Word tokens: {hypothesis_results['word_tokens']}\")\n",
    "        print(f\"Char tokens: {hypothesis_results['char_tokens']}\")\n",
    "        print(f\"Golden tokens: {hypothesis_results['golden_tokens']}\")\n",
    "        print(f\"RoBERTa embedding shape: {hypothesis_results['roberta_embeddings'].shape}\")\n",
    "        print(f\"BERT embedding shape: {hypothesis_results['bert_embeddings'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
