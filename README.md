# Custom Tokenization and NLI Performance

By:

- Yme Boland
- Shane Siwpersad
- Eduard Köntöş
- Hugo Voorheijen
- Julius Bijkerk

## Project Overview

This repository is part of a group project for the Logic and Language course at Utrecht Univerity, taught by Dr. Lasha Abzianidze PhD.
We will be exploring the impact of different tokenization techniques on Natural Language Inference (NLI), using three pre-trained models like BART, DistilRoBERTa, and MiniLM2.

## Objectives

- Investigate how different tokenization strategies affect NLI model performance.
- Use the SNLI dataset (and possibly othe NLI datasets, like MNLI) for testing and comparison.

## Repository Structure

* `plots` - bar plots to help visualize the differences in model performance.
* `results` - raw `jsonl` result files containing the predicted label, accuracy, and probabilities related to the prediction process. Each used model has its own folder, where each tokenization approach we defined in our project is used for that model.
* `shap_analysis` - code to evaluate the feature importance of tokens from DistilRoBERTa and MiniLM2 on SNLI.
* `tokenization_methods` - main module containg all of the different tokenization approaches.
* `undertrained` - contains the reports and results generated by the code of Sander Land and Max Bartolo. 2024. [Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models](https://aclanthology.org/2024.emnlp-main.649/)
* Other:
    * `eval_*.ipynb` - notebooks to evaluate the models' performance 
    * `eval_all.sh` - run all experiments concurrently
    * `custom_*.py` - custom python classes for tokenizers and models to interface with our customised tokenization methods.
    * `prediction_utilities.py` - methods to obtain labels from a given model.
    * `tokenization_verification.py` - verify that no errors occur durring tokenization and that the resulting tokens fit within the context window of a mdoel.

## Environment Setup

req file

## Running the code
